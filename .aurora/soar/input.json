{
  "prompt": "Original Query: how do i enhance agent recovery for aur soar to detect it early without waiting\n\nSynthesized Answer:\nTo enhance agent recovery for `aur soar` with early failure detection, you need to implement a multi-layered approach based on the existing architecture:\n## Current State (Agent: code-analyzer)\nThe system currently detects failures through:\n- **Error pattern monitoring**: 11 regex patterns check stderr in real-time (~500ms intervals), detecting API/auth errors within 5 seconds\n- **Circuit breaker**: Opens after 2 failures, blocks spawns for 120 seconds before retry\n- **Process timeout**: Fixed 300-second timeout for all agents\n- **Retry logic**: Maximum 2 retries with fallback to direct LLM\nThe main limitation is that you're waiting up to 300 seconds for timeout on stuck agents, with no early detection of \"slow but working\" versus \"actually stuck\" scenarios.\n## Recommended Implementation (Agent: reliability-engineer)\nImplement these components to detect failures earlier:\n**1. Enhanced Circuit Breaker** (`packages/spawner/src/aurora_spawner/circuit_breaker.py`):\n- Reduce failure threshold from 2 to 3 for more aggressive protection\n- Make timeout configurable (currently hardcoded at 120s)\n- Add HALF_OPEN state testing for gradual recovery\n**2. Pre-flight Health Checks** (`packages/spawner/src/aurora_spawner/health.py`):\n- Startup check: Verify agent spawns within 5 seconds\n- Memory access check: Confirm project database is accessible\n- LLM availability check: Test endpoint connectivity before spawn\n- Run checks before each spawn to fail fast on infrastructure issues\n**3. Integration with Spawner**:\nModify `packages/spawner/src/aurora_spawner/spawner.py` to:\n- Run health checks before spawning (fails in <10 seconds vs 300-second timeout)\n- Check process alive status 2 seconds after spawn\n- Wrap spawn calls with circuit breaker protection\n**4. Configuration** (`.aurora/config.yaml`):\n```yaml\nresilience:\ncircuit_breaker:\nfailure_threshold: 3\ntimeout_seconds: 60\nhealth_checks:\nstartup_timeout: 5.0\nenabled: true\n```\n**5. CLI Commands**:\n```bash\naur spawn tasks.md                    # Normal with health checks\naur spawn tasks.md --no-health-check  # Skip checks\naur spawn status                      # View circuit breaker state\n```\n## Expected Improvements\n- **Detection speed**: 5-10 seconds (health checks) vs 300 seconds (timeout)\n- **Resource protection**: Circuit breaker prevents repeated failed spawns\n- **User experience**: Clear error messages like \"Agent health check failed: memory_access - Memory index empty\" instead of generic timeout\n- **System stability**: Automatic backoff when agents repeatedly fail\n## Implementation Priority\n1. Add health check system (highest impact, fastest detection)\n2. Integrate with spawner pre-flight checks\n3. Make circuit breaker thresholds configurable\n4. Add `aur spawn status` command for observability\n5. Implement heartbeat mechanism for long-running tasks (deferred - requires more complex changes)\nThe key insight is that early detection doesn't require waiting - you can verify agent readiness (startup, memory, LLM access) in seconds before attempting spawn, rather than discovering failures after 300-second timeouts.\n\nAgent Outputs:\n\nAgent 0 (code-analyzer):\nSummary: # SOAR Orchestrator Error Handling and Agent Execution Flow Analysis\n\n## Executive Summary\n\nThe SOAR orchestrator implements a sophisticated multi-layer error handling system with graceful degradation patterns. The system prioritizes resilience through circuit breakers, retry logic, fallback mechanisms, and progressive timeout handling across its 7-phase pipeline.\n\n## Control Flow Architecture\n\n### Phase Execution Pattern\n\n```\nexecute() \u2192 [Phase 1-7] \u2192 _phase8_respond() \u2192 return\n    \u2193\n    Exception handlers at multiple levels:\n    1. Budget pre-check (BudgetExceededError)\n    2. Individual phase try-catch\n    3. Top-level exception handler\n```\n\n### Error Recovery Hierarchy\n\n**Level 1: Budget Protection** (Lines 243-268)\n- Pre-execution budget check using `CostTracker.check_budget()`\n- Hard rejection if budget exceeded\n- Soft warning if approaching limit\n- No graceful degradation - raises `BudgetExceededError` immediately\n\n**Level 2: Phase-Level Resilience** (Lines 512-534, 573-596, 598-633)\n- Each phase wrapped in try-catch\n- Defaults to safe fallback values on error\n- Example (Phase 1 Assess):\n  ```python\n  except Exception as e:\n      return {\"complexity\": \"MEDIUM\", \"confidence\": 0.0, \"_error\": str(e)}\n  ```\n- Execution continues even if phase fails\n- Error stored in phase metadata for debugging\n\n**Level 3: Decomposition Failure Handling** (Lines 296-299, 1011-1051)\n- Special handler for Phase 3 failures\n- Checks `phase3_result.get(\"_error\")` before proceeding\n- Returns partial results with user-friendly error message\n- Does NOT propagate exception - graceful degradation\n\n**Level 4: Verification Failure with Retry** (Lines 355-403)\n- Automatic retry on verification failure (single retry)\n- Generates feedback from issues list\n- Re-runs decomposition with retry feedback\n- If retry fails, calls `_handle_verification_failure()`\n- Returns partial response rather than raising\n\n**Level 5: Agent Execution Recovery** (collect.py:179-401)\n**Multi-layered recovery system:**\n\n1. **Circuit Breaker** (Lines 660-673, circuit_breaker.py:38-187)\n   - Tracks failures per agent across invocations\n   - Opens circuit after 2 failures (configurable)\n   - Skips known-broken agents for 120s (reset timeout)\n   - Half-open state for testing recovery\n   - **Key behavior**: Fails fast on known issues\n\n2. **Retry with Progressive Policy** (spawner.py:346-475)\n   - Default: max_retries=2, fallback_to_llm=True\n   - Configurable via PoliciesEngine\n   - Retry count tracked per agent\n   - Circuit breaker checked before each retry\n   - **Key behavior**: Handles transient failures\n\n3. **Fallback to Direct LLM** (collect.py:449-470)\n   - Triggered after all retries exhausted\n   - Removes agent persona, uses raw LLM\n   - Metadata tracks fallback usage\n   - Success recorded but doesn't reset circuit breaker\n   - **Key behavior**: Ensures progress even when agent broken\n\n4. **Critical vs Non-Critical Handling** (collect.py:367-383)\n   - Critical subgoals: Raises `RuntimeError` on hard failure\n   - Non-critical: Logs error, continues execution\n   - Soft errors (timeout, rate limit) never stop execution\n   - **Key behavior**: Differentiated failure response\n\n**Level 6: Spawner-Level Early Failure Detection** (spawner.py:42-238)\n- Monitors stderr for error patterns in real-time\n- Kills process immediately on API/auth errors\n- Pattern matching for 10+ error types (rate limit, connection errors, etc.)\n- Progressive timeout: extends if stdout activity detected\n- **Key behavior**: Fast-fail on infrastructure issues\n\n**Level 7: Top-Level Exception Handler** (Lines 476-481, 1053-1086)\n- Catches all unhandled exceptions\n- Wraps in error response with zero confidence\n- Logs full exception stack\n- Returns structured error to user\n- **Key behavior**: No exception escapes to user\n\n## Failure Point Analysis\n\n### 1. Budget Exceeded\n**Location**: orchestrator.py:243-268  \n**Behavior**: Immediate hard failure  \n**Propagation**: Raises `BudgetExceededError` to caller  \n**Rationale**: Financial safety - no degradation allowed  \n\n### 2. Assessment Phase Failure (Phase 1)\n**Location**: orchestrator.py:512-534  \n**Behavior**: Defaults to \"MEDIUM\" complexity, 0.0 confidence  \n**Propagation**: Continues to Phase 2  \n**Impact**: May use sub-optimal retrieval budget  \n\n### 3. Retrieval Phase Failure (Phase 2)\n**Location**: orchestrator.py:573-596  \n**Behavior**: Returns empty chunks, logs error  \n**Propagation**: Continues to Phase 3 with no context  \n**Impact**: Decomposition works with general knowledge only  \n\n### 4. Decomposition Phase Failure (Phase 3)\n**Location**: orchestrator.py:598-633  \n**Behavior**: Returns empty subgoals with error marker  \n**Propagation**: Detected at line 296, calls `_handle_decomposition_failure()`  \n**Impact**: Returns error message to user with partial metadata  \n**Recovery**: None - decomposition is critical  \n\n### 5. Verification Failure (Phase 4)\n**Location**: orchestrator.py:355-403  \n**Behavior**: Single automatic retry with feedback  \n**Propagation**: If retry fails, calls `_handle_verification_failure()`  \n**Impact**: Returns partial results with verification feedback  \n**Recovery**: One retry attempt  \n\n### 6. Agent Execution Failure (Phase 5)\n**Multi-level recovery cascade:**\n\n**6a. Circuit Breaker Skip**  \n**Location**: spawner.py:380-412  \n**Behavior**: Skip agent, go directly to fallback LLM  \n**Propagation**: Returns SpawnResult with `fallback=True`  \n**Impact**: Faster failure, prevents wasted retries  \n\n**6b. Spawn Error Pattern Detection**  \n**Location**: spawner.py:142-168  \n**Behavior**: Kill process immediately on API error  \n**Propagation**: Returns SpawnResult with `success=False`  \n**Impact**: Saves time on known infrastructure failures  \n\n**6c. Retry Attempts**  \n**Location**: spawner.py:417-446  \n**Behavior**: Up to 2 retries (configurable)  \n**Propagation**: Each failure recorded to circuit breaker  \n**Impact**: Handles transient failures (network blips, etc.)  \n\n**6d. Fallback to LLM**  \n**Location**: spawner.py:448-470  \n**Behavior**: Remove agent persona, use direct LLM  \n**Propagation**: Returns SpawnResult with `fallback=True, original_agent=X`  \n**Impact**: Graceful degradation - answer still generated  \n\n**6e. Critical Subgoal Failure**  \n**Location**: collect.py:367-383  \n**Behavior**: Raises RuntimeError only for hard failures on critical subgoals  \n**Propagation**: Aborts entire SOAR execution  \n**Impact**: Query fails with error message  \n**Note**: Soft errors (timeout, rate limit) never trigger this  \n\n**6f. Non-Critical Subgoal Failure**  \n**Location**: collect.py:385, 619-624  \n**Behavior**: Creates AgentOutput with `success=False`  \n**Propagation**: Continues to next subgoal  \n**Impact**: Partial results in synthesis  \n\n### 7. Synthesis Phase Failure (Phase 6)\n**Location**: orchestrator.py:700-733  \n**Behavior**: Returns error synthesis with 0.0 confidence  \n**Propagation**: Continues to Phase 7 (record)  \n**Impact**: Error message passed to response formatting  \n\n### 8. Simple Path Failure\n**Location**: orchestrator.py:928-936  \n**Behavior**: Returns error synthesis with 0.0 confidence  \n**Propagation**: Continues to Phase 8 (respond)  \n**Impact**: Graceful error message for simple queries  \n\n## Timeout Handling\n\n### Agent-Level Timeouts\n**Configuration**: `DEFAULT_AGENT_TIMEOUT = 300s` (5 minutes)  \n**Override**: Via `task.timeout` parameter  \n**Progressive behavior**:\n- Initial check every 0.5s\n- Extends if stdout activity detected\n- Kills process on timeout\n- Returns error with \"Process timed out\" message\n\n### Spawn-Level Early Exit\n**Pattern detection**: Real-time stderr monitoring  \n**Kill triggers**:\n- Rate limit (429)\n- Connection errors (ECONNRESET)\n- API errors\n- Authentication failures\n- Quota exceeded\n\n### Soft vs Hard Timeouts\n**Soft timeout** (Lines 371-383):\n- Detected by error message patterns\n- Does NOT abort critical subgoals\n- Logs at debug level, continues silently\n- Patterns: \"timeout\", \"no activity\", \"timed out\", \"circuit open\"\n\n**Hard timeout**:\n- Non-soft errors on critical subgoals\n- Raises RuntimeError\n- Aborts entire execution\n\n## Metadata Tracking\n\n### Error Propagation in Metadata\nAll errors stored in `_phase_metadata` dict with:\n- `_error: str | None` field per phase\n- `_timing_ms: float` for performance analysis\n- Error details in top-level `error_details` key\n\n### Agent Execution Metadata\n```python\nexecution_metadata = {\n    \"duration_ms\": int,\n    \"exit_code\": int,\n    \"fallback\": bool,\n    \"retry_count\": int,\n    \"original_agent\": str | None,\n    \"spawned\": bool,  # If ad-hoc spawn\n}\n```\n\n### Fallback Tracking\n- `CollectResult.fallback_agents: list[str]` - IDs of agents that used fallback\n- `execution_metadata[\"fallback_count\"]` - Total fallback count\n- `execution_metadata[\"spawn_count\"]` - Ad-hoc spawns count\n\n## Key Design Patterns\n\n### 1. Graceful Degradation\n- Phases fail independently\n- Execution continues with defaults\n- Final confidence score reflects quality\n\n### 2. Circuit Breaker Pattern\n- Per-agent failure tracking\n- Automatic skip after threshold\n- Half-open testing for recovery\n- Prevents cascading failures\n\n### 3. Retry with Exponential Backoff (Implicit)\n- Fixed max_retries=2 (no backoff)\n- Circuit breaker provides implicit \"backoff\"\n- Reset timeout acts as cooldown period\n\n### 4. Fallback Chain\n```\nAgent attempt \u2192 Retry (2x) \u2192 Fallback to LLM \u2192 Graceful failure message\n```\n\n### 5. Differentiated Error Handling\n- **Critical errors**: Budget, critical subgoals\n- **Recoverable errors**: Phase failures, verification, agent failures\n- **Soft errors**: Timeouts, rate limits (never abort critical paths)\n\n### 6. Error Context Preservation\n- Error messages preserved through chain\n- Metadata tracks retry counts, fallback usage\n- User gets detailed error information\n\n---\n\n## Suggested Agent Specification\n\n**Agent ID**: `error-flow-analyst`\n\n**Role/Title**: Error Flow and Fault Tolerance Analyst\n\n**Goal Description**: Analyze error handling patterns, fault tolerance mechanisms, and failure recovery strategies in codebases. Trace error propagation paths and identify failure points with recovery logic.\n\n**Key Capabilities**:\n1. **Control Flow Tracing** - Map exception propagation paths through call stacks, identifying catch blocks, error transformations, and recovery points\n2. **Failure Mode Analysis** - Catalog failure scenarios (timeout, retry, fallback, circuit breaker) and their handling strategies with differentiation between hard/soft failures\n3. **Resilience Pattern Detection** - Identify retry logic, circuit breakers, fallback chains, graceful degradation, and timeout handling mechanisms\n4. **Error Metadata Extraction** - Parse error tracking systems, metadata propagation, logging patterns, and diagnostic information flow\n5. **Recovery Strategy Evaluation** - Assess robustness of error handling, identify recovery gaps, and recommend improvements to fault tolerance\n\nConfidence: 0.85\n\nAgent 1 (code-analyzer):\nSummary: Perfect! Now I have comprehensive information. Let me compile the complete analysis:\n\n# Agent Failure Detection and Timeout Analysis\n\n## Executive Summary\n\nAgent failures are detected through a **multi-layered system** involving error pattern recognition, circuit breakers, and progressive timeouts. Detection happens at **multiple points** in the execution pipeline with varying timeout periods depending on failure mode.\n\n---\n\n## 1. Failure Detection Mechanisms\n\n### 1.1 Error Pattern Detection (Real-time, <5s)\n**Location**: `packages/spawner/src/aurora_spawner/spawner.py:22-34`\n\n11 regex patterns monitor stderr in real-time:\n- Rate limit errors (`rate.?limit`, `\\b429\\b`)\n- Connection failures (`connection.?(refused|reset|error)`, `ECONNRESET`)\n- API errors (`API.?error`, `model.?not.?available`, `quota.?exceeded`)\n- Authentication (`authentication.?failed`, `invalid.?api.?key`, `unauthorized`, `forbidden`)\n\n**Detection Speed**: Stderr is read line-by-line, process killed immediately on pattern match (typically <5 seconds)\n\n**Code**: `spawner.py:142-168`\n```python\nasync def read_stderr():\n    while True:\n        chunk = await process.stderr.read(1024)\n        buffer += chunk.decode(errors=\"ignore\")\n        while \"\\n\" in buffer:\n            line, buffer = buffer.split(\"\\n\", 1)\n            error = _check_error_patterns(line)\n            if error:\n                error_detected = error\n                return  # Stop reading, kill process\n```\n\n### 1.2 Circuit Breaker (Immediate Skip)\n**Location**: `packages/spawner/src/aurora_spawner/circuit_breaker.py:38-202`\n\n**Configuration**:\n- `failure_threshold`: 2 failures (default)\n- `reset_timeout`: 120 seconds (default)\n\n**States**:\n- **CLOSED**: Normal operation, allow spawns\n- **OPEN**: Agent failing, skip spawns entirely (return immediately without spawning)\n- **HALF_OPEN**: After 120s, allow one test request\n\n**Behavior**:\n```\nFailure #1 \u2192 State: CLOSED, continue\nFailure #2 \u2192 State: OPEN, skip all spawns\nWait 120s  \u2192 State: HALF_OPEN, allow test\nSuccess    \u2192 State: CLOSED\nFailure    \u2192 State: OPEN again\n```\n\n**Detection Speed**: Instant (pre-spawn check at `spawner.py:380-412`)\n\n### 1.3 Process Timeout\n**Location**: `spawner.py:36-39, 129, 185-190`\n\n**Primary Timeout**: \n- Default: **300 seconds** (5 minutes)\n- Configurable via `SpawnTask.timeout` parameter\n- Set at CLI level: `spawn.py:320, 370` (hardcoded to 300s)\n\n**Timeout Loop**:\n```python\nwhile process.returncode is None:\n    if error_detected:\n        process.kill()  # Early termination\n        break\n    elapsed = time.time() - start_time\n    if elapsed > timeout_seconds:\n        process.kill()\n        error_detected = f\"Process timed out after {timeout_seconds} seconds\"\n        break\n    await asyncio.sleep(0.5)  # Check every 500ms\n```\n\n---\n\n## 2. Timeout Configuration Matrix\n\n| Component | Timeout Value | Location | Notes |\n|-----------|--------------|----------|-------|\n| **SpawnTask default** | 300s | `spawner/models.py:13` | Per-task configurable |\n| **CLI spawn command** | 300s | `spawn.py:320, 370` | Hardcoded in CLI |\n| **Circuit breaker reset** | 120s | `circuit_breaker.py:55` | Time until HALF_OPEN |\n| **LLM API timeout** | 30s | `config/defaults.json:17` | For direct LLM calls |\n| **Database timeout** | 5s | `config/defaults.json:8` | SQLite operations |\n| **Stderr check interval** | 500ms | `spawner.py:193` | Polling interval |\n\n---\n\n## 3. Retry and Fallback Logic\n\n**Location**: `spawner.py:346-475`\n\n**Retry Configuration**:\n- `max_retries`: 2 (default, configurable)\n- Total agent attempts: `max_retries + 1 = 3` attempts\n- Fallback to LLM: enabled by default\n\n**Execution Flow**:\n```\nAttempt 1 \u2192 Failure \u2192 Circuit records failure #1\nAttempt 2 \u2192 Failure \u2192 Circuit records failure #2 \u2192 Circuit OPENS\nAttempt 3 \u2192 Circuit check \u2192 SKIP (circuit open)\nFallback  \u2192 Spawn with agent=None (direct LLM)\n```\n\n**Per-Attempt Tracking**: Each failed attempt increments circuit breaker counter (`spawner.py:444`)\n\n---\n\n## 4. Failure Detection Timeline\n\n### Fast Path (API/Connection Errors)\n```\n0ms      \u2192 Spawn process\n~100ms   \u2192 First stderr output with error pattern\n~500ms   \u2192 Error pattern detected, process.kill() called\n~1-5s    \u2192 Process terminated, failure recorded\n\u2192 Circuit breaker: failure #1 or #2\n```\n\n### Slow Path (Timeout)\n```\n0s       \u2192 Spawn process\n...\n300s     \u2192 Timeout exceeded, process.kill() called\n~301s    \u2192 Process terminated, failure recorded\n\u2192 Circuit breaker: failure recorded\n```\n\n### Circuit Open Path (Immediate)\n```\n0ms      \u2192 Check circuit_breaker.should_skip()\n0ms      \u2192 Returns (True, \"Circuit open: 2 failures...\")\n0ms      \u2192 Skip to fallback (if enabled) OR return error\n\u2192 No spawn occurs\n```\n\n---\n\n## 5. Key Files Summary\n\n| File | Role | Key Values |\n|------|------|------------|\n| `spawner/spawner.py:36-39` | DEFAULT_TIMEOUT constant | 300s |\n| `spawner/spawner.py:22-34` | ERROR_PATTERNS | 11 patterns |\n| `spawner/spawner.py:346-475` | Retry logic | max_retries=2 |\n| `circuit_breaker.py:55` | Circuit config | threshold=2, timeout=120s |\n| `spawn.py:320, 370` | CLI timeout | 300s (hardcoded) |\n| `models.py:13` | Task default | timeout=300 |\n\n---\n\n## 6. Current Limitations\n\n1. **No adaptive timeout**: 300s is fixed, doesn't adapt to task complexity\n2. **Circuit breaker not configurable**: Hardcoded 2 failures, 120s reset\n3. **No partial progress detection**: Can't distinguish \"slow but working\" from \"stuck\"\n4. **Retry delays**: No exponential backoff between retries\n5. **No separate network vs processing timeout**: Same 300s for both\n\n---\n\n# Agent Specification Recommendation\n\nBased on this analysis, I recommend creating a specialized agent:\n\n## Agent ID: `execution-flow-analyst`\n\n## Role/Title: Execution Flow & Resilience Analyst\n\n## Goal Description\nAnalyzes async execution paths, timeout configurations, and failure handling mechanisms in distributed agent systems. Specializes in tracing how errors propagate through retry logic, circuit breakers, and fallback strategies.\n\n## Key Capabilities\n\n1. **Timeout Audit & Mapping**: Systematically identifies all timeout configurations across system layers (process, network, API, database) and maps their interaction effects\n\n2. **Failure Path Tracing**: Traces execution paths from initial failure detection through retry logic, circuit breaker state changes, and ultimate fallback decisions with precise timing analysis\n\n3. **Resilience Pattern Analysis**: Evaluates circuit breaker configurations, retry strategies, and error pattern detection mechanisms for effectiveness and potential race conditions\n\n4. **Performance Bottleneck Identification**: Identifies where async operations block unnecessarily, where timeouts are too aggressive/conservative, and where failure detection could be optimized\n\n5. **Configuration Impact Assessment**: Analyzes how timeout and retry configurations cascade through the system, identifying potential deadlock scenarios or resource exhaustion patterns\n\nThis agent would be particularly useful for PRD-0029 style reliability improvements and system health diagnostics.\n\nConfidence: 0.85\n\nAgent 2 (reliability-engineer):\nSummary: # Circuit Breaker & Health Check Pattern for Early Agent Failure Detection\n\n## Overview\n\nImplements a resilience layer for `aur spawn` and `aur soar` to detect and respond to agent failures before they cascade, protecting system resources and user experience.\n\n## Core Components\n\n### 1. Circuit Breaker Pattern\n\n```python\n# packages/spawner/src/aurora_spawner/circuit_breaker.py\n\nfrom enum import Enum\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nfrom typing import Optional, Callable, Any\nimport time\n\nclass CircuitState(Enum):\n    CLOSED = \"closed\"      # Normal operation\n    OPEN = \"open\"          # Failures exceeded threshold\n    HALF_OPEN = \"half_open\"  # Testing if service recovered\n\n@dataclass\nclass CircuitBreakerConfig:\n    failure_threshold: int = 3  # Failures before opening\n    success_threshold: int = 2  # Successes to close from half-open\n    timeout_seconds: int = 60   # Time before trying half-open\n    \n@dataclass\nclass CircuitBreaker:\n    name: str\n    config: CircuitBreakerConfig = field(default_factory=CircuitBreakerConfig)\n    state: CircuitState = CircuitState.CLOSED\n    failure_count: int = 0\n    success_count: int = 0\n    last_failure_time: Optional[datetime] = None\n    \n    def call(self, func: Callable, *args, **kwargs) -> Any:\n        \"\"\"Execute function with circuit breaker protection.\"\"\"\n        if self.state == CircuitState.OPEN:\n            if self._should_attempt_reset():\n                self.state = CircuitState.HALF_OPEN\n                self.success_count = 0\n            else:\n                raise CircuitBreakerOpenError(\n                    f\"Circuit breaker '{self.name}' is OPEN. \"\n                    f\"Retry after {self._time_until_retry()}s\"\n                )\n        \n        try:\n            result = func(*args, **kwargs)\n            self._on_success()\n            return result\n        except Exception as e:\n            self._on_failure()\n            raise\n    \n    def _on_success(self):\n        if self.state == CircuitState.HALF_OPEN:\n            self.success_count += 1\n            if self.success_count >= self.config.success_threshold:\n                self.state = CircuitState.CLOSED\n                self.failure_count = 0\n        elif self.state == CircuitState.CLOSED:\n            self.failure_count = 0\n    \n    def _on_failure(self):\n        self.failure_count += 1\n        self.last_failure_time = datetime.now()\n        \n        if self.failure_count >= self.config.failure_threshold:\n            self.state = CircuitState.OPEN\n    \n    def _should_attempt_reset(self) -> bool:\n        if not self.last_failure_time:\n            return False\n        timeout = timedelta(seconds=self.config.timeout_seconds)\n        return datetime.now() - self.last_failure_time > timeout\n    \n    def _time_until_retry(self) -> float:\n        if not self.last_failure_time:\n            return 0.0\n        elapsed = (datetime.now() - self.last_failure_time).total_seconds()\n        return max(0.0, self.config.timeout_seconds - elapsed)\n\nclass CircuitBreakerOpenError(Exception):\n    \"\"\"Raised when circuit breaker is open.\"\"\"\n    pass\n```\n\n### 2. Health Check System\n\n```python\n# packages/spawner/src/aurora_spawner/health.py\n\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import Optional, Dict, List\nfrom enum import Enum\n\nclass HealthStatus(Enum):\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\"\n    UNHEALTHY = \"unhealthy\"\n\n@dataclass\nclass HealthCheck:\n    name: str\n    status: HealthStatus\n    message: str\n    timestamp: datetime\n    latency_ms: Optional[float] = None\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\nclass AgentHealthMonitor:\n    \"\"\"Monitor agent health with multiple check types.\"\"\"\n    \n    def __init__(self, agent_id: str):\n        self.agent_id = agent_id\n        self.checks: Dict[str, HealthCheck] = {}\n    \n    def check_startup(self, timeout_seconds: float = 5.0) -> HealthCheck:\n        \"\"\"Verify agent can start within timeout.\"\"\"\n        start = time.time()\n        try:\n            # Attempt minimal agent initialization\n            # Check if CLI process spawns, returns help text, exits cleanly\n            result = subprocess.run(\n                [\"aur\", \"spawn\", \"--help\"],\n                capture_output=True,\n                timeout=timeout_seconds,\n                text=True\n            )\n            latency = (time.time() - start) * 1000\n            \n            if result.returncode != 0:\n                return HealthCheck(\n                    name=\"startup\",\n                    status=HealthStatus.UNHEALTHY,\n                    message=f\"Startup failed: {result.stderr[:200]}\",\n                    timestamp=datetime.now(),\n                    latency_ms=latency\n                )\n            \n            return HealthCheck(\n                name=\"startup\",\n                status=HealthStatus.HEALTHY,\n                message=\"Agent startup successful\",\n                timestamp=datetime.now(),\n                latency_ms=latency\n            )\n        except subprocess.TimeoutExpired:\n            return HealthCheck(\n                name=\"startup\",\n                status=HealthStatus.UNHEALTHY,\n                message=f\"Startup timeout after {timeout_seconds}s\",\n                timestamp=datetime.now(),\n                latency_ms=timeout_seconds * 1000\n            )\n    \n    def check_memory_access(self) -> HealthCheck:\n        \"\"\"Verify agent can access project memory.\"\"\"\n        start = time.time()\n        try:\n            from aurora_core.store.memory import MemoryStore\n            store = MemoryStore()\n            # Quick metadata query - doesn't load full index\n            stats = store.get_statistics()\n            latency = (time.time() - start) * 1000\n            \n            if not stats or stats.get(\"indexed_files\", 0) == 0:\n                return HealthCheck(\n                    name=\"memory_access\",\n                    status=HealthStatus.DEGRADED,\n                    message=\"Memory index empty or uninitialized\",\n                    timestamp=datetime.now(),\n                    latency_ms=latency\n                )\n            \n            return HealthCheck(\n                name=\"memory_access\",\n                status=HealthStatus.HEALTHY,\n                message=f\"Memory accessible ({stats['indexed_files']} files)\",\n                timestamp=datetime.now(),\n                latency_ms=latency,\n                metadata=stats\n            )\n        except Exception as e:\n            return HealthCheck(\n                name=\"memory_access\",\n                status=HealthStatus.UNHEALTHY,\n                message=f\"Memory access failed: {str(e)[:200]}\",\n                timestamp=datetime.now(),\n                latency_ms=(time.time() - start) * 1000\n            )\n    \n    def check_llm_availability(self) -> HealthCheck:\n        \"\"\"Verify LLM endpoint is reachable.\"\"\"\n        start = time.time()\n        try:\n            from aurora_cli.llm.factory import get_llm_client\n            client = get_llm_client()\n            # Minimal prompt to test connectivity\n            response = client.generate(\"test\", max_tokens=1)\n            latency = (time.time() - start) * 1000\n            \n            # High latency warning\n            status = HealthStatus.HEALTHY\n            if latency > 5000:\n                status = HealthStatus.DEGRADED\n            \n            return HealthCheck(\n                name=\"llm_availability\",\n                status=status,\n                message=f\"LLM responsive (latency: {latency:.0f}ms)\",\n                timestamp=datetime.now(),\n                latency_ms=latency\n            )\n        except Exception as e:\n            return HealthCheck(\n                name=\"llm_availability\",\n                status=HealthStatus.UNHEALTHY,\n                message=f\"LLM unreachable: {str(e)[:200]}\",\n                timestamp=datetime.now(),\n                latency_ms=(time.time() - start) * 1000\n            )\n    \n    def run_all_checks(self) -> Dict[str, HealthCheck]:\n        \"\"\"Run all health checks and return results.\"\"\"\n        self.checks[\"startup\"] = self.check_startup()\n        self.checks[\"memory_access\"] = self.check_memory_access()\n        self.checks[\"llm_availability\"] = self.check_llm_availability()\n        return self.checks\n    \n    def is_healthy(self) -> bool:\n        \"\"\"Overall health status.\"\"\"\n        if not self.checks:\n            self.run_all_checks()\n        return all(\n            check.status != HealthStatus.UNHEALTHY \n            for check in self.checks.values()\n        )\n```\n\n### 3. Integration with Spawner\n\n```python\n# packages/spawner/src/aurora_spawner/spawner.py\n\nfrom .circuit_breaker import CircuitBreaker, CircuitBreakerOpenError, CircuitBreakerConfig\nfrom .health import AgentHealthMonitor, HealthStatus\n\nclass AgentSpawner:\n    def __init__(self):\n        # Per-agent circuit breakers\n        self.circuit_breakers: Dict[str, CircuitBreaker] = {}\n        self.health_monitors: Dict[str, AgentHealthMonitor] = {}\n        \n    def spawn_agent(self, agent_id: str, task_file: Path, **kwargs) -> AgentProcess:\n        \"\"\"Spawn agent with health checks and circuit breaker.\"\"\"\n        \n        # 1. Pre-flight health check\n        monitor = self._get_health_monitor(agent_id)\n        health_checks = monitor.run_all_checks()\n        \n        unhealthy = [\n            name for name, check in health_checks.items() \n            if check.status == HealthStatus.UNHEALTHY\n        ]\n        if unhealthy:\n            raise AgentHealthError(\n                f\"Agent {agent_id} health check failed: {', '.join(unhealthy)}\\n\"\n                f\"Run 'aur doctor' for diagnostics\"\n            )\n        \n        # 2. Circuit breaker protection\n        breaker = self._get_circuit_breaker(agent_id)\n        try:\n            return breaker.call(self._spawn_agent_internal, agent_id, task_file, **kwargs)\n        except CircuitBreakerOpenError as e:\n            raise AgentSpawnError(\n                f\"Agent {agent_id} circuit breaker OPEN: {e}\\n\"\n                f\"Recent failures detected. System protecting resources.\"\n            ) from e\n    \n    def _spawn_agent_internal(self, agent_id: str, task_file: Path, **kwargs) -> AgentProcess:\n        \"\"\"Internal spawn logic - wrapped by circuit breaker.\"\"\"\n        # Existing spawn implementation\n        process = self._start_process(agent_id, task_file, **kwargs)\n        \n        # Early failure detection: check process health after 2s\n        time.sleep(2)\n        if not process.is_alive() or process.returncode is not None:\n            raise AgentSpawnError(\n                f\"Agent {agent_id} died immediately after spawn. \"\n                f\"Check logs at {process.log_path}\"\n            )\n        \n        return process\n    \n    def _get_circuit_breaker(self, agent_id: str) -> CircuitBreaker:\n        if agent_id not in self.circuit_breakers:\n            self.circuit_breakers[agent_id] = CircuitBreaker(\n                name=agent_id,\n                config=CircuitBreakerConfig(\n                    failure_threshold=3,\n                    success_threshold=2,\n                    timeout_seconds=60\n                )\n            )\n        return self.circuit_breakers[agent_id]\n    \n    def _get_health_monitor(self, agent_id: str) -> AgentHealthMonitor:\n        if agent_id not in self.health_monitors:\n            self.health_monitors[agent_id] = AgentHealthMonitor(agent_id)\n        return self.health_monitors[agent_id]\n```\n\n### 4. CLI Integration\n\n```python\n# packages/cli/src/aurora_cli/commands/spawn.py\n\n@spawn.command()\n@click.argument(\"task_file\", type=click.Path(exists=True))\n@click.option(\"--no-health-check\", is_flag=True, help=\"Skip pre-flight health checks\")\ndef execute(task_file: str, no_health_check: bool):\n    \"\"\"Execute tasks with circuit breaker protection.\"\"\"\n    spawner = AgentSpawner()\n    \n    try:\n        if not no_health_check:\n            click.echo(\"Running pre-flight health checks...\")\n            monitor = spawner._get_health_monitor(\"spawn\")\n            checks = monitor.run_all_checks()\n            \n            for name, check in checks.items():\n                icon = \"\u2713\" if check.status == HealthStatus.HEALTHY else \"\u26a0\" if check.status == HealthStatus.DEGRADED else \"\u2717\"\n                click.echo(f\"  {icon} {name}: {check.message}\")\n            \n            if not monitor.is_healthy():\n                raise click.ClickException(\"Health checks failed. Use --no-health-check to override.\")\n        \n        agent = spawner.spawn_agent(\"spawn\", Path(task_file))\n        click.echo(f\"Agent spawned: {agent.pid}\")\n        \n    except CircuitBreakerOpenError as e:\n        raise click.ClickException(str(e))\n    except AgentHealthError as e:\n        raise click.ClickException(str(e))\n\n@spawn.command()\ndef status():\n    \"\"\"Show circuit breaker and health status.\"\"\"\n    spawner = AgentSpawner()\n    \n    click.echo(\"Circuit Breaker Status:\")\n    for agent_id, breaker in spawner.circuit_breakers.items():\n        click.echo(f\"  {agent_id}: {breaker.state.value} (failures: {breaker.failure_count})\")\n    \n    click.echo(\"\\nHealth Check Status:\")\n    for agent_id, monitor in spawner.health_monitors.items():\n        if monitor.checks:\n            overall = \"healthy\" if monitor.is_healthy() else \"unhealthy\"\n            click.echo(f\"  {agent_id}: {overall}\")\n```\n\n## Configuration\n\n```yaml\n# .aurora/config.yaml\n\nresilience:\n  circuit_breaker:\n    failure_threshold: 3      # Failures before opening\n    success_threshold: 2      # Successes to close\n    timeout_seconds: 60       # Time before retry\n  \n  health_checks:\n    startup_timeout: 5.0      # Max agent startup time\n    memory_check: true        # Verify memory access\n    llm_check: true           # Verify LLM availability\n    check_interval: 300       # Re-check every 5 minutes\n```\n\n## Testing\n\n```python\n# tests/unit/spawner/test_circuit_breaker.py\n\ndef test_circuit_opens_after_threshold():\n    breaker = CircuitBreaker(\"test\", CircuitBreakerConfig(failure_threshold=2))\n    \n    # First failure\n    with pytest.raises(Exception):\n        breaker.call(lambda: 1/0)\n    assert breaker.state == CircuitState.CLOSED\n    \n    # Second failure - opens circuit\n    with pytest.raises(Exception):\n        breaker.call(lambda: 1/0)\n    assert breaker.state == CircuitState.OPEN\n\ndef test_circuit_half_open_after_timeout():\n    config = CircuitBreakerConfig(failure_threshold=1, timeout_seconds=1)\n    breaker = CircuitBreaker(\"test\", config)\n    \n    # Trigger opening\n    with pytest.raises(Exception):\n        breaker.call(lambda: 1/0)\n    assert breaker.state == CircuitState.OPEN\n    \n    # Wait for timeout\n    time.sleep(1.1)\n    \n    # Next call should attempt half-open\n    result = breaker.call(lambda: \"success\")\n    assert breaker.state == CircuitState.HALF_OPEN\n    assert result == \"success\"\n\ndef test_health_check_detects_missing_memory():\n    monitor = AgentHealthMonitor(\"test\")\n    # Mock empty memory\n    check = monitor.check_memory_access()\n    assert check.status in [HealthStatus.DEGRADED, HealthStatus.UNHEALTHY]\n```\n\n## Usage Examples\n\n```bash\n# Normal spawn with health checks\naur spawn tasks.md\n\n# Skip health checks (emergency override)\naur spawn tasks.md --no-health-check\n\n# Check circuit breaker status\naur spawn status\n\n# Force circuit breaker reset\naur spawn reset-breakers\n```\n\n## Benefits\n\n1. **Early Detection**: Health checks catch issues before spawn attempts\n2. **Resource Protection**: Circuit breakers prevent cascading failures\n3. **User Experience**: Clear failure messages with diagnostic hints\n4. **System Stability**: Automatic backoff when agents repeatedly fail\n5. **Observability**: Status command shows resilience state\n\n---\n\n## Formal Agent Specification\n\n**Agent ID**: `resilience-engineer`\n\n**Role/Title**: Reliability & Resilience Engineer\n\n**Goal**: Design and implement resilience patterns (circuit breakers, health checks, graceful degradation) to ensure system stability under failure conditions.\n\n**Key Capabilities**:\n1. **Circuit Breaker Design** - Implement state-based failure protection with configurable thresholds and automatic recovery\n2. **Health Check Architecture** - Design multi-layered health monitoring (startup, resource access, external dependencies)\n3. **Graceful Degradation** - Build fallback mechanisms that maintain core functionality during partial failures\n4. **Failure Analysis** - Diagnose cascading failures and recommend resilience improvements\n5. **Observability Integration** - Expose circuit breaker states and health metrics for monitoring and debugging\n\nConfidence: 0.85\n\nAgent 3 (systems-programmer):\nSummary: \nConfidence: 0.0\n\nAgent 4 (systems-programmer):\nSummary: \nConfidence: 0.0\n\nAgent 5 (code-analyzer):\nSummary: \nConfidence: 0.0\n\nAgent 6 (test-engineer):\nSummary: \nConfidence: 0.0\n\nAgent 7 (observability-engineer):\nSummary: \nConfidence: 0.0\n\n\nVerify this synthesis and provide quality assessment in JSON format.",
  "system": "You are a quality verifier for synthesized responses.\n\nYour task is to verify that a synthesis meets quality standards:\n\n1. COHERENCE: Is the synthesis well-structured, logical, and clear?\n2. COMPLETENESS: Does it address all aspects of the original query?\n3. FACTUALITY: Are all claims properly grounded in agent outputs?\n\nScore each dimension from 0.0 to 1.0, then calculate overall_score as the average.\n\nYou MUST respond with valid JSON only. Use this exact format:\n{\n  \"coherence\": 0.0-1.0,\n  \"completeness\": 0.0-1.0,\n  \"factuality\": 0.0-1.0,\n  \"overall_score\": 0.0-1.0,\n  \"issues\": [\"list\", \"of\", \"issues\"],\n  \"suggestions\": [\"list\", \"of\", \"improvements\"]\n}\n\nYou MUST respond with valid JSON only. Do not include markdown code blocks, explanations, or any text outside the JSON object.",
  "phase": "unknown",
  "tool": "claude"
}
