# Embedding Loading Strategy - Quick Reference

**Last Updated**: 2025-01-26

---

## TL;DR

âœ… **Lazy imports work**: 280ms import time (target: <2s)
âœ… **Background loading works**: 6.4s wall time vs 20s synchronous
âœ… **User-perceived latency**: 2.3s (88.5% faster than 20s)

**Strategy is highly effective** - no changes needed.

---

## Performance Summary

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| Module import time | <2s | 280ms | âœ… 86% better |
| User-perceived latency | <5s | 2.3s | âœ… 54% better |
| Cold search (BG load) | <10s | 6.4s | âœ… 36% better |
| Warm search | <2s | 2-3s | âš ï¸ 0-50% over |

---

## Three-Layer Architecture

```
Layer 1: LIGHTWEIGHT CHECK (0.37ms)
  â””â”€ is_model_cached_fast() [filesystem only]

Layer 2: LAZY IMPORTS (280ms)
  â””â”€ aurora_cli.config, aurora_core.store.sqlite
  â””â”€ NO torch/transformers imported

Layer 3: BACKGROUND LOADING (11.2s, parallel)
  â””â”€ BackgroundModelLoader.start_loading()
  â””â”€ Loads in background thread (daemon)
```

---

## User Experience Flow

```bash
$ aur mem search "authentication"

â³ Loading embedding model in background...
âš¡ Fast mode (BM25+activation) - embedding model loading in background

[Results in 2.3s - BM25+activation mode]

Total wall time: 6.4s (includes background loading)
```

---

## Timing Breakdown

| Phase | Time | % Total | Blocks Main Thread? |
|-------|------|---------|---------------------|
| CLI imports | 280ms | 4.4% | âœ… Yes |
| Cache check | 0.37ms | 0.0% | âœ… Yes |
| Start BG load | <1ms | 0.0% | âœ… Yes (spawn only) |
| Store init | 3ms | 0.0% | âœ… Yes |
| BM25 index load | 400ms | 6.3% | âœ… Yes |
| Database query | 120ms | 1.9% | âœ… Yes |
| BM25 scoring | 1,200ms | 18.8% | âœ… Yes |
| Result format | 100ms | 1.6% | âœ… Yes |
| **RESULTS RETURNED** | **2.1s** | **33%** | **USER SEES RESULTS** |
| BG model load | 4.3s | 67% | âŒ No (parallel) |
| **TOTAL** | **6.4s** | **100%** | |

---

## Comparison: Before vs After Epic 1

| Scenario | Before | After | Improvement |
|----------|--------|-------|-------------|
| Module import | 30s | 280ms | 99.1% faster |
| Cold search (sync) | 20s | 20s | No change |
| Cold search (BG) | N/A | 6.4s | New feature |
| User-perceived | 20s | 2.3s | 88.5% faster |
| Warm search | 4-5s | 2-3s | 40-50% faster |

---

## Key Implementation Details

### 1. Lightweight Cache Check

```python
# NO torch/transformers import - just filesystem
def is_model_cached_fast(model_name: str) -> bool:
    cache_path = HF_CACHE_DIR / f"models--{model_name.replace('/', '--')}"
    return (cache_path / "snapshots").exists()
```

**Time**: 0.37ms

---

### 2. Lazy Import Pattern

```python
# Module level: NO imports
_torch = None
_SentenceTransformer = None

def _lazy_import() -> bool:
    global _torch, _SentenceTransformer
    if _HAS_SENTENCE_TRANSFORMERS is not None:
        return _HAS_SENTENCE_TRANSFORMERS

    try:
        import torch as torch_module
        from sentence_transformers import SentenceTransformer as ST
        _torch = torch_module
        _SentenceTransformer = ST
        _HAS_SENTENCE_TRANSFORMERS = True
    except ImportError:
        _HAS_SENTENCE_TRANSFORMERS = False

    return _HAS_SENTENCE_TRANSFORMERS
```

**First call**: 11.2s (loads torch + transformers)
**Subsequent calls**: <0.01ms (cached)

---

### 3. Background Model Loader

```python
class BackgroundModelLoader:
    def start_loading(self, model_name: str) -> None:
        """Non-blocking - returns immediately."""
        self._thread = threading.Thread(target=self._load_model, daemon=True)
        self._thread.start()

    def _load_model(self) -> None:
        """Runs in background thread."""
        provider = EmbeddingProvider(model_name=self._model_name)
        provider.preload_model()  # 11.2s in background
```

**start_loading()**: <1ms (thread spawn)
**_load_model()**: 11.2s (in background, doesn't block)

---

## Fallback Strategies

| Scenario | Behavior | Fallback |
|----------|----------|----------|
| Model not cached | Skip BG load | BM25-only, suggest indexing |
| Import error | Skip BG load | BM25-only, suggest `pip install aurora-cli[ml]` |
| BG load fails | Log error | BM25-only, show warning |
| Wait timeout (>60s) | Return None | BM25-only, show timeout |

**Result**: âœ… Search ALWAYS works, never crashes

---

## Cache Effectiveness

### HybridRetriever Cache

- **Key**: `(db_path, config_hash)`
- **TTL**: 1800s (30 minutes)
- **Size**: 10 instances
- **Impact**: 30-40% cold search speedup

### ActivationEngine Cache

- **Key**: `db_path`
- **Scope**: Singleton per database
- **Impact**: 40-50% warm search speedup

### QueryEmbeddingCache

- **Key**: Normalized query text
- **Size**: 100 entries (LRU)
- **Hit rate**: 60%+ in SOAR multi-phase operations
- **Impact**: 99.8% faster on cache hit (<5ms vs 2.9s)

### BM25 Index Persistence

- **Path**: `.aurora/indexes/bm25_index.pkl`
- **Load time**: 405ms
- **Rebuild time**: 9.7s
- **Impact**: 24x speedup

---

## Common Questions

### Q: Why 6.4s and not 20s?

**A**: Background loading. Model loads in parallel (11.2s) while main thread returns results (2.1s).

```
Main thread:   [========] 2.1s â†’ RESULTS RETURNED
Background:    [==================] 11.2s
Wall time:     2.1s + 4.3s = 6.4s (overlap)
```

---

### Q: Why not instant on warm search?

**A**: BM25 scoring (1.2s) is the bottleneck. Optimization in Epic 2:
- Pre-tokenized storage during indexing
- Expected: 2-3s â†’ 500ms (75% faster)

---

### Q: What if model isn't cached?

**A**: BM25-only mode works fine:
- Search time: ~2s (no embedding overhead)
- Quality: Lower recall, but still useful
- Guidance: Suggests `aur mem index .` to download model

---

### Q: What about GPU acceleration?

**A**: Not yet implemented (P3 priority):
- Model load: 11.2s â†’ 1-2s (with GPU)
- Query embedding: 2.9s â†’ 200ms
- Trade-off: Requires CUDA, not portable

---

## Optimization Opportunities

### Already Implemented âœ…

1. Lazy imports (280ms, not 30s)
2. Background loading (6.4s, not 20s)
3. Lightweight cache check (<1ms)
4. HybridRetriever cache (30-40% speedup)
5. ActivationEngine cache (40-50% speedup)
6. BM25 index persistence (24x speedup)
7. Query embedding cache (60%+ hit rate)

### Future Work ðŸ”¶

1. **P1: BM25 pre-tokenization** (25% total speedup)
   - Store tokens during indexing
   - Avoid re-tokenizing during search
   - Expected: 20s â†’ 15s

2. **P2: Model quantization** (70% model load speedup)
   - Reduce model size (88MB â†’ 22MB)
   - Expected: 11.2s â†’ 3-4s

3. **P3: GPU acceleration** (85% inference speedup)
   - Use CUDA if available
   - Expected: 2.9s â†’ 200ms

---

## Code Locations

| Component | File | Lines |
|-----------|------|-------|
| Lightweight cache check | `aurora_context_code/model_cache.py` | 33-68 |
| Lazy imports | `aurora_context_code/semantic/embedding_provider.py` | 36-82 |
| Background loader | `aurora_context_code/semantic/model_utils.py` | 207-419 |
| MemoryRetriever integration | `aurora_cli/memory/retrieval.py` | 106-174 |
| CLI entry point | `aurora_cli/commands/memory.py` | 40-60, 493-494 |

---

## Testing

### Unit Tests

```bash
# Test lazy imports (no torch loaded)
pytest tests/unit/context_code/test_embedding_provider.py::test_lazy_import -v

# Test background loader
pytest tests/unit/context_code/semantic/test_model_utils.py -v
```

### Performance Tests

```bash
# Test import time (<2s target)
pytest tests/performance/test_soar_startup_performance.py::test_import_time -v

# Test background loading effectiveness
pytest tests/performance/test_embedding_loading.py -v
```

### Manual Testing

```bash
# Test cold search with background loading
rm -rf ~/.cache/huggingface  # Clear cache
time aur mem search "authentication" --limit 5

# Test warm search (model cached)
time aur mem search "database" --limit 5
```

---

## Metrics to Monitor

| Metric | Target | Command |
|--------|--------|---------|
| Import time | <2s | `python -m timeit -n 1 -r 1 "import aurora_context_code.model_cache"` |
| Cache check time | <10ms | `time aur mem search "test"` (check first line) |
| User-perceived latency | <5s | `time aur mem search "test" --limit 5` (to first result) |
| Wall time (BG load) | <10s | `time aur mem search "test" --limit 5` (total) |
| Warm search | <2s | `time aur mem search "test" --limit 5` (second run) |

---

## Troubleshooting

### Issue: Import takes >2s

**Cause**: torch/transformers imported at module level

**Solution**: Check for eager imports, use `_lazy_import()`

---

### Issue: Background loading not starting

**Cause**: Model not cached or import error

**Debug**:
```python
from aurora_context_code.model_cache import is_model_cached_fast
print(is_model_cached_fast())  # Should be True
```

---

### Issue: User-perceived latency >5s

**Cause**: BM25 scoring bottleneck (1.2s) + overhead

**Solution**: Implement pre-tokenization (Epic 2)

---

## References

- **Full Analysis**: `docs/analysis/EMBEDDING_LOADING_STRATEGY_ANALYSIS.md`
- **Performance Profile**: `docs/analysis/MEMORY_SEARCH_PERFORMANCE_PROFILE.md`
- **Optimization Plan**: `docs/analysis/MEMORY_SEARCH_OPTIMIZATION_PLAN.md`

---

**Document Status**: âœ… Complete
**Last Profiled**: 2025-01-26 (v0.11.x)
