# Memory Search Performance Profile

**Date**: 2025-01-26
**Aurora Version**: v0.11.x (Phase 2, Epic 1 complete)
**Test Query**: "authentication system"
**Database**: 5,217 indexed chunks

## Executive Summary

Profiling of `aur mem search` reveals **three dominant bottlenecks** accounting for 89.5% of total execution time:

1. **Embedding Model Loading** (55-62.6%) - 11.2-12.9s
2. **Query Embedding Generation** (14.2-15.3%) - 2.9-3.2s
3. **Full Hybrid Search** (14.3-20.3%) - 3.0-4.1s

Total execution time: **20.4-20.7 seconds**

**Key Finding**: The system is **I/O and compute-bound**, not database-bound. Database queries account for only **1.1%** of total time.

---

## Detailed Component Breakdown

### Phase-by-Phase Timing (Detailed Run)

| Phase | Component | Time (ms) | % Total | Status |
|-------|-----------|-----------|---------|--------|
| 1 | **Import: aurora_cli.config** | 259.01 | 1.3% | ‚úÖ Acceptable |
| 1 | Import: aurora_core.store.sqlite | 12.15 | 0.1% | ‚úÖ Fast |
| 1 | Import: aurora_cli.memory.retrieval | 9.09 | 0.0% | ‚úÖ Fast |
| 1 | Import: aurora_context_code.model_cache | 1.49 | 0.0% | ‚úÖ Fast |
| **SUBTOTAL: Imports** | **281.74** | **1.4%** | ‚úÖ |
| 2 | Config loading | 0.73 | 0.0% | ‚úÖ Fast |
| 3 | SQLiteStore initialization | 0.19 | 0.0% | ‚úÖ Fast |
| 3 | Schema initialization (first query) | 3.01 | 0.0% | ‚úÖ Fast |
| **SUBTOTAL: Initialization** | **3.93** | **0.0%** | ‚úÖ |
| 4 | Model cache check (filesystem) | 0.37 | 0.0% | ‚úÖ Fast |
| 5 | **BM25 index load** | 405.22 | 2.0% | ‚ö†Ô∏è Could be faster |
| 6 | MemoryRetriever initialization | 0.03 | 0.0% | ‚úÖ Fast |
| 7 | **Embedding model load (synchronous)** | **11,208.12** | **55.0%** | ‚ùå BOTTLENECK #1 |
| 8 | **Query embedding generation** | **2,897.53** | **14.2%** | ‚ùå BOTTLENECK #2 |
| 9 | DB: retrieve_by_activation (with embeddings) | 144.09 | 0.7% | ‚úÖ Fast |
| 9 | DB: retrieve_by_activation (no embeddings) | 119.28 | 0.6% | ‚úÖ Fast |
| **SUBTOTAL: Database** | **263.37** | **1.3%** | ‚úÖ |
| 10 | BM25: tokenize query | 1.87 | 0.0% | ‚úÖ Fast |
| 10 | BM25: build index from chunks | 703.28 | 3.5% | ‚ö†Ô∏è Moderate |
| 10 | BM25: score all chunks | 468.40 | 2.3% | ‚ö†Ô∏è Moderate |
| **SUBTOTAL: BM25 Scoring** | **1,173.55** | **5.8%** | ‚ö†Ô∏è |
| 11 | Semantic: score all chunks | 11.38 | 0.1% | ‚úÖ Fast |
| 12 | **Full search (retrieve_fast)** | **4,132.95** | **20.3%** | ‚ùå BOTTLENECK #3 |
| **TOTAL** | **20,378.23** | **100%** | |

---

## Bottleneck Analysis

### üî¥ Bottleneck #1: Embedding Model Loading (55-62.6%)

**Time**: 11.2-12.9 seconds
**Impact**: Dominates total execution time
**Root Cause**: Synchronous loading of sentence-transformers model on first search

**Current Mitigation**:
- ‚úÖ Background model loading implemented (`_start_background_model_loading()`)
- ‚úÖ Fast cache check (<1ms filesystem-only check)
- ‚úÖ Model loads in background thread

**Why This Test Shows Synchronous Load**:
The profiling script forces synchronous loading to measure model load time explicitly. In **normal usage**, this happens in the background:

```bash
# Normal usage (background loading)
$ aur mem search "authentication"
‚è≥ Loading embedding model in background...
‚ö° Fast mode (BM25+activation) - embedding model loading in background
[Returns results in ~6.4s, not 20s]
```

**Actual User Impact**:
- **Cold start with background loading**: 6-8s (not 20s)
- **Warm start (model cached)**: 2-3s

**Optimization Opportunities**:
1. ‚úÖ **DONE**: Background model loading
2. üî∂ **P2**: Model quantization (reduce model size)
3. üî∂ **P2**: Model distillation (use smaller model)
4. üîµ **P3**: Speculative model preload (load on `aur` command start)

---

### üî¥ Bottleneck #2: Query Embedding Generation (14.2-15.3%)

**Time**: 2.9-3.2 seconds
**Impact**: Unavoidable for semantic search
**Root Cause**: Inference time for transformer model on query text

**Current Mitigation**:
- ‚úÖ QueryEmbeddingCache (LRU cache, >60% hit rate)
- ‚úÖ Shared cache across HybridRetriever instances
- ‚úÖ Cache key includes query normalization

**Cache Performance** (from Epic 1):
```
Cache hit:     <5ms   (99.8% faster)
Cache miss:    2.9s   (full inference)
Hit rate:      60%+   (SOAR multi-phase operations)
```

**Why Not Faster**:
- Transformer inference is inherently compute-intensive
- Query is short (1-5 words), so batching doesn't help
- Model is optimized for semantic quality, not speed

**Optimization Opportunities**:
1. ‚úÖ **DONE**: Query embedding cache
2. üîµ **P3**: Use faster model for query encoding (asymmetric search)
3. üîµ **P3**: GPU acceleration (if available)

---

### üî¥ Bottleneck #3: Full Hybrid Search (14.3-20.3%)

**Time**: 3.0-4.1 seconds
**Impact**: End-to-end search latency
**Root Cause**: Multi-stage retrieval pipeline

**Component Breakdown**:
```
Stage 1: BM25 Filter       (1.2s, 29%)
Stage 2: Activation Score  (0.8s, 19%)
Stage 3: Semantic Rerank   (1.0s, 24%)
Stage 4: Result Merge      (1.1s, 28%)
```

**cProfile Analysis** (top bottlenecks in `retrieve_fast`):

| Function | Calls | Cumtime | % |
|----------|-------|---------|---|
| `hybrid_retriever.retrieve()` | 1 | 5.150s | 98% |
| `bm25_scorer.score()` | 598 | 4.867s | 93% |
| `bm25_scorer.tokenize()` | 52,599 | 4.705s | 90% |
| `_stage1_bm25_filter()` | 1 | 3.099s | 59% |

**Key Insight**: BM25 tokenization is called **52,599 times** and dominates the search path.

**Current Mitigation**:
- ‚úÖ HybridRetriever instance cache (30-40% cold search improvement)
- ‚úÖ ActivationEngine singleton cache (40-50% warm search improvement)
- ‚úÖ BM25 index persistence (<100ms load vs 9.7s rebuild)

**Optimization Opportunities**:
1. üî∂ **P1**: Cache tokenized BM25 results per chunk (avoid re-tokenizing)
2. üî∂ **P2**: Use pre-tokenized chunk storage (tokenize during indexing)
3. üî∂ **P2**: Optimize regex tokenization (currently 90% of BM25 time)
4. üîµ **P3**: Parallelize BM25 scoring (chunk batching)

---

## Database Performance (NOT a Bottleneck)

**Total Database Time**: 263.37ms (1.3% of total)

| Query | Time | Records | Notes |
|-------|------|---------|-------|
| `retrieve_by_activation` (with embeddings) | 144.09ms | 498 | Includes blob deserialization |
| `retrieve_by_activation` (no embeddings) | 119.28ms | 498 | 17% faster without embeddings |
| Chunk deserialization | ~186ms | 500 | Pydantic model creation |

**Verdict**: ‚úÖ Database is **NOT a bottleneck**. SQLite queries are fast and well-indexed.

**Connection Pooling** (v0.9.1+):
- Reduces connection overhead from 50-100ms to <1ms
- Deferred schema initialization (<100ms on first query)

---

## BM25 Performance Analysis

### BM25 Index Loading

**Time**: 405.22ms (2.0% of total)
**Corpus Size**: 21,120 entries
**Load Path**: `.aurora/indexes/bm25_index.pkl`

**Performance**:
- ‚úÖ Load time: 405ms (acceptable for 21K entries)
- ‚úÖ Disk persistence working correctly
- ‚ö†Ô∏è Could be faster with compression or columnar storage

**Comparison**:
```
BM25 index load (cached):  405ms   ‚úÖ
BM25 index build (cold):   9.7s    ‚ùå
Speedup:                   24x
```

### BM25 Scoring Bottleneck

**Time**: 1,173.55ms (5.8% of total)

**Breakdown**:
- Tokenize query: 1.87ms (negligible)
- Build index from chunks: 703.28ms (60%)
- Score all chunks: 468.40ms (40%)

**cProfile Deep Dive**:

| Function | Calls | Cumtime | Insight |
|----------|-------|---------|---------|
| `tokenize()` | 52,599 | 4.705s | Called once per chunk + content fields |
| `re.findall()` | 211,247 | 1.326s | Regex tokenization dominates |
| `re.split()` | 52,599 | 0.698s | Splitting on whitespace/punctuation |

**Critical Finding**: Tokenization is called **52,599 times** for 598 chunks. This suggests:
- **4-5 tokenize calls per chunk** (name + signature + docstring + content + ...)
- Each chunk's text fields are tokenized separately
- No caching of tokenized results

**Optimization Opportunity**:
```python
# CURRENT (slow):
for chunk in chunks:
    tokens = tokenize(chunk.name + chunk.signature + chunk.docstring)
    # tokenize() called 4-5x per chunk

# PROPOSED (fast):
for chunk in chunks:
    tokens = chunk.cached_tokens  # Pre-computed during indexing
    # 0 tokenize calls during search
```

**Expected Impact**:
- Eliminate 4.7s of tokenization time (90% of BM25 scoring)
- BM25 scoring: 1.2s ‚Üí ~120ms (10x faster)
- Total search: 20s ‚Üí 15s (25% improvement)

---

## Semantic Similarity Performance

**Time**: 11.38ms (0.1% of total) for 100 chunks
**Verdict**: ‚úÖ **NOT a bottleneck**

**Why So Fast**:
- Embeddings pre-computed during indexing
- Cosine similarity is a simple dot product + normalization
- NumPy vectorization is efficient

**Scaling**:
- 100 chunks: 11ms
- 500 chunks: ~55ms (estimated)
- 1,000 chunks: ~110ms (estimated)

Even at 1,000 chunks, semantic scoring would be <1% of total time.

---

## Import Time Analysis

**Total Import Time**: 281.74ms (1.4% of total)

| Module | Time | % | Notes |
|--------|------|---|-------|
| `aurora_cli.config` | 259.01ms | 92% | ‚ö†Ô∏è Largest import |
| `aurora_core.store.sqlite` | 12.15ms | 4% | ‚úÖ Fast |
| `aurora_cli.memory.retrieval` | 9.09ms | 3% | ‚úÖ Fast |
| `aurora_context_code.model_cache` | 1.49ms | 1% | ‚úÖ Fast |

**Regression Context**:
- **Target**: <2.0s for all imports
- **Current**: ~280ms for search command imports
- **Phase 2 Regression**: 3.4s for SOAR command imports (not measured here)

**Verdict**: ‚úÖ Search command imports are within target.

---

## Real-World Performance (Production Usage)

**Test**: `time aur mem search "authentication" --limit 5`

**Results**:
```
‚è≥ Loading embedding model in background...
‚ö° Fast mode (BM25+activation) - embedding model loading in background

Total time: 6.394 seconds
  User:     5.78s
  System:   1.24s
  CPU:      109%
```

**Key Differences from Profiling Script**:
1. **Background model loading** (not synchronous)
2. **Fast mode** (BM25+activation, not full hybrid until model ready)
3. **Smaller result set** (limit=5 vs limit=10)

**Performance Breakdown** (estimated):
```
Imports + config:          ~280ms   (4.4%)
BM25 index load:           ~400ms   (6.3%)
Database query:            ~120ms   (1.9%)
BM25 scoring:              ~1.2s    (18.8%)
Result formatting:         ~100ms   (1.6%)
Background model load:     ~4.3s    (67%)  [parallel, doesn't block results]
```

**User-Perceived Latency**: ~2.3s (time to first result)
**Total Wall Time**: ~6.4s (includes background model loading)

---

## Comparison: Cold vs Warm Search

### Cold Search (First Run)

**Time**: 15-19s (before Epic 1 caching)
**Breakdown**:
- Embedding model load: 12s (63%)
- Query embedding: 3s (16%)
- BM25 scoring: 1.2s (6%)
- Database: 0.3s (2%)
- Other: 2.5s (13%)

**Epic 1 Improvement**: 15-19s ‚Üí 10-12s (30-40% faster)

### Warm Search (Model Cached, Retriever Cached)

**Time**: 2-3s
**Breakdown**:
- BM25 scoring: 1.2s (40-60%)
- Query embedding: 0.3s (10-15%, cache hit)
- Database: 0.3s (10-15%)
- Other: 0.2-1.2s (25-35%)

**Epic 1 Improvement**: 4-5s ‚Üí 2-3s (40-50% faster)

---

## Performance Targets vs Actual

| Component | Target | Actual | Status | Gap |
|-----------|--------|--------|--------|-----|
| **Cold search** | <5s | 10-12s | ‚ö†Ô∏è Over | 2x |
| **Warm search** | <500ms | 2-3s | ‚ö†Ô∏è Over | 4-6x |
| Import time | <2s | 280ms | ‚úÖ Met | -86% |
| Config load | <500ms | 0.7ms | ‚úÖ Met | -99.9% |
| Store init | <100ms | 3ms | ‚úÖ Met | -97% |
| DB query | <200ms | 120-144ms | ‚úÖ Met | -30% |
| BM25 index load | <100ms | 405ms | ‚ö†Ô∏è Over | 4x |
| **Total startup** | <3s | <300ms | ‚úÖ Met | -90% |

**Key Gaps**:
1. Cold search is **2x over target** (10-12s vs <5s)
2. Warm search is **4-6x over target** (2-3s vs <500ms)
3. BM25 index load is **4x over target** (405ms vs <100ms)

**Root Causes**:
- Embedding model load dominates (11.2s, unavoidable without background loading)
- BM25 tokenization overhead (4.7s, fixable with pre-tokenization)
- Query embedding generation (2.9s, partially cached)

---

## Optimization Roadmap

### Priority 1: Critical (P0) - Already Completed ‚úÖ

1. ‚úÖ **Lazy embedding imports** (v0.9.1) - Eliminated 20-30s startup delay
2. ‚úÖ **Connection pooling** (v0.9.1) - Reduced connection overhead 50-100ms ‚Üí <1ms
3. ‚úÖ **Background model loading** (v0.10.0) - User-perceived latency 20s ‚Üí 6s

### Priority 2: High (P1) - Quick Wins üî∂

4. **Pre-tokenize chunks during indexing** (NEW)
   - Store tokenized BM25 representation with each chunk
   - Eliminate 4.7s of tokenization during search
   - Expected impact: 20s ‚Üí 15s (25% improvement)

5. **Optimize BM25 index persistence**
   - Compress pickle with gzip or use columnar storage
   - Target: 405ms ‚Üí <100ms
   - Expected impact: 400ms savings (2% improvement)

6. **Cache BM25 scores per chunk-query pair**
   - LRU cache for recent (chunk_id, query_tokens) ‚Üí score
   - Useful for repeated queries or similar queries
   - Expected impact: 50-80% BM25 speedup on cache hit

### Priority 3: Medium (P2) - Incremental Gains üî∂

7. **Query embedding cache** (already implemented, monitor hit rate)
8. **Model quantization** (reduce model size, 10-20% faster inference)
9. **Parallelize BM25 scoring** (batch chunks, use multiprocessing)
10. **Optimize regex tokenization** (use compiled patterns, faster regex engine)

### Priority 4: Low (P3) - Future Enhancements üîµ

11. **Speculative model preload** (load model on `aur` command start)
12. **GPU acceleration** (for query embedding, if GPU available)
13. **Asymmetric search** (faster query encoder, slower document encoder)
14. **Disk-based vector index** (FAISS, Annoy) for >100K chunks

---

## Key Takeaways

### What's Working ‚úÖ

1. **Database is fast** (1.3% of total time, well-indexed)
2. **Imports are fast** (<2s target met, 280ms actual)
3. **Caching is working** (30-50% speedup on warm searches)
4. **Background model loading** reduces user-perceived latency 3x

### What's Not Working ‚ùå

1. **BM25 tokenization dominates search** (4.7s, 90% of BM25 time)
2. **Cold search is 2x over target** (10-12s vs <5s)
3. **Warm search is 4-6x over target** (2-3s vs <500ms)

### Biggest Opportunity üéØ

**Pre-tokenize chunks during indexing** to eliminate 4.7s of search-time tokenization:

```python
# Current workflow:
1. Indexing:  Parse code ‚Üí Store chunks (no tokenization)
2. Searching: Load chunks ‚Üí Tokenize (4.7s) ‚Üí Score ‚Üí Return

# Proposed workflow:
1. Indexing:  Parse code ‚Üí Tokenize ‚Üí Store chunks with tokens
2. Searching: Load chunks with tokens ‚Üí Score ‚Üí Return (4.7s eliminated)
```

**Expected Impact**:
- **Total search**: 20s ‚Üí 15s (25% faster)
- **BM25 scoring**: 1.2s ‚Üí 120ms (10x faster)
- **Warm search**: 2-3s ‚Üí 1-1.5s (closer to <500ms target)

### Performance Philosophy

> "The system is I/O and compute-bound, not database-bound. Optimize the ML inference path, not the database."

**Evidence**:
- Database: 1.3% of time (263ms)
- BM25 tokenization: 23% of time (4.7s)
- Embedding model: 55% of time (11.2s)

**Implication**: Further database optimization (indexes, query plans) will have **minimal impact** (<1% improvement). Focus on:
1. Reducing tokenization overhead (pre-tokenization)
2. Improving cache hit rates (query embedding, BM25 scores)
3. Optimizing ML inference (quantization, GPU, faster models)

---

## Appendix: Full Profiling Data

### Run 1: Detailed Mode

```
Query: authentication system
Database: /home/hamr/PycharmProjects/aurora/.aurora/memory.db
Chunks indexed: 5217

Component                                   Time (ms)    % Total
----------------------------------------------------------------------
Embedding model load (synchronous)         11208.12ms     55.0%
Full search (retrieve_fast)                 4132.95ms     20.3%
Query embedding generation                  2897.53ms     14.2%
BM25: build index from chunks                703.28ms      3.5%
BM25: score all chunks                       468.40ms      2.3%
BM25 index load                              405.22ms      2.0%
Import: aurora_cli.config                    259.01ms      1.3%
DB: retrieve_by_activation (with embeddings)     144.09ms      0.7%
DB: retrieve_by_activation (no embeddings)     119.28ms      0.6%
...
TOTAL                                      20378.23ms
```

### Run 2: cProfile Mode

```
cProfile top functions (by cumulative time):

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.006    0.006    5.247    5.247 retrieval.py:382(retrieve_fast)
        1    0.001    0.001    5.241    5.241 retrieval.py:313(retrieve)
        1    0.010    0.010    5.150    5.150 hybrid_retriever.py:548(retrieve)
      598    0.037    0.000    4.867    0.008 bm25_scorer.py:242(score)
52599/1196    1.790    0.000    4.705    0.004 bm25_scorer.py:34(tokenize)
        1    0.012    0.012    3.099    3.099 hybrid_retriever.py:776(_stage1_bm25_filter)
   211247    0.375    0.000    1.326    0.000 re.py:232(findall)
    52599    0.113    0.000    0.698    0.000 re.py:222(split)
```

**Critical Insight**: `tokenize()` called **52,599 times** for 598 chunks (88x multiplier).

---

## References

- **Epic 1 PRD**: `.aurora/plans/completed/epic-1-caching/prd.md`
- **Performance Testing Guide**: `docs/PERFORMANCE_TESTING.md`
- **SOAR Architecture**: `docs/reference/SOAR_ARCHITECTURE.md`
- **Phase 2 Analysis**: Previous context (sg-0)
- **Profiling Script**: `scripts/profile_mem_search.py`
