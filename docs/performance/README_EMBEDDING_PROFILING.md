# Embedding Load Time Profiling Infrastructure

> **Comprehensive tooling for measuring, tracking, and optimizing embedding model load times**

## Quick Start

### Run profiling now (5 runs, warm start)
```bash
cd /home/hamr/PycharmProjects/aurora
python3 scripts/profile_embedding_load.py
```

### Interactive workflow example
```bash
./examples/profiling_workflow_example.sh
```

## What This Provides

This profiling infrastructure gives you:

1. âœ… **Detailed load time measurements** - Stage-by-stage breakdown of model loading
2. âœ… **Statistical confidence** - Multiple runs with mean, median, stdev, min, max
3. âœ… **Memory profiling** - Track memory usage during loading
4. âœ… **Performance targets** - Clear benchmarks for acceptable performance
5. âœ… **Baseline comparison** - Detect improvements or regressions
6. âœ… **Actionable recommendations** - Data-driven optimization suggestions
7. âœ… **CI/CD integration** - Automated regression detection

## Tools Included

| Tool | Purpose | Usage |
|------|---------|-------|
| **profile_embedding_load.py** | Main profiling tool | `python3 scripts/profile_embedding_load.py` |
| **run_embedding_profile.sh** | Shell wrapper | `./scripts/run_embedding_profile.sh` |
| **check_performance_regression.py** | CI/CD regression check | `python3 scripts/check_performance_regression.py` |
| **profiling_workflow_example.sh** | Interactive tutorial | `./examples/profiling_workflow_example.sh` |

## Documentation

- **ğŸ“– [Complete Documentation](docs/performance/embedding_load_profiling.md)** - Methodology, targets, optimization strategies
- **ğŸ“ [Quick Reference](scripts/README_PROFILING.md)** - Commands and examples
- **ğŸ“Š [Summary](PROFILING_SUMMARY.md)** - Deliverables overview

## Common Tasks

### Establish Baseline
```bash
# First time - establish baseline
python3 scripts/profile_embedding_load.py \
  --output reports/embedding_load_warm_baseline.json
```

### Before/After Comparison
```bash
# Before optimization
python3 scripts/profile_embedding_load.py --output reports/before.json

# ... make optimization changes ...

# After optimization - compare with before
python3 scripts/profile_embedding_load.py --baseline reports/before.json
```

### Test Cold Start (First-time User Experience)
```bash
python3 scripts/profile_embedding_load.py --cold-start
```

### Compare Different Models
```bash
# Default model (MiniLM-L6)
python3 scripts/profile_embedding_load.py --model all-MiniLM-L6-v2

# Faster model (MiniLM-L3)
python3 scripts/profile_embedding_load.py --model paraphrase-MiniLM-L3-v2

# Higher quality model (MPNet)
python3 scripts/profile_embedding_load.py --model all-mpnet-base-v2
```

### CI/CD Integration
```bash
# Profile and check for regression
python3 scripts/profile_embedding_load.py --output reports/current.json
python3 scripts/check_performance_regression.py \
  --current reports/current.json \
  --baseline reports/baseline.json \
  --threshold 1.2  # Fail if >20% slower
```

## What Gets Measured

### Stage Breakdown
1. **Import Time** - Loading sentence-transformers and dependencies
2. **Model Init Time** - Instantiating and loading model weights
3. **First Encode Time** - First embedding generation (warmup)
4. **Memory Usage** - Memory delta from start to finish

### Statistics Per Stage
- Mean (average across runs)
- Median (middle value)
- Standard deviation (variability)
- Min/Max (range)

## Performance Targets

| Metric | Target (Warm) | Current Status |
|--------|---------------|----------------|
| Total Load Time | â‰¤ 5.0s | Run profiling to measure |
| Import Time | â‰¤ 1.0s | Run profiling to measure |
| Model Init | â‰¤ 3.0s | Run profiling to measure |
| First Encode | â‰¤ 200ms | Run profiling to measure |
| Memory Usage | â‰¤ 500MB | Run profiling to measure |

## Example Output

```
================================================================================
PROFILING RESULTS
================================================================================

ğŸ“‹ Summary (WARM START)
   Model: all-MiniLM-L6-v2
   Runs: 5 (5 successful)

â±ï¸  Timing Breakdown (mean Â± stdev)
--------------------------------------------------------------------------------
   Total Load Time:       3.28 Â± 0.15s  (range: 3.10-3.45s)
   â”œâ”€ Import Time:        0.84 Â± 0.08s  (range: 0.75-0.92s)
   â”œâ”€ Model Init Time:    2.31 Â± 0.12s  (range: 2.18-2.44s)
   â””â”€ First Encode Time:  0.13 Â± 0.02s  (range: 0.11-0.15s)

ğŸ’¾ Memory Usage
--------------------------------------------------------------------------------
   Memory Delta:  243.8 Â± 5.2MB

ğŸ¯ Performance vs Targets
--------------------------------------------------------------------------------
   âœ“ Total Load             3.28s / 5.0s target
   âœ“ Import                 0.84s / 1.0s target
   âœ“ Model Init             2.31s / 3.0s target
   âœ“ First Encode           130ms / 200ms target
   âœ“ Memory Usage           244MB / 500MB target

ğŸ’¡ Recommendations
--------------------------------------------------------------------------------
   âœ“ Warm start time (3.28s) meets target (5.0s)
   âœ“ Acceptable memory usage (244MB)
```

## Workflow

### 1. Initial Setup
```bash
# Establish baseline (run once)
python3 scripts/profile_embedding_load.py \
  --output reports/embedding_load_warm_baseline.json
```

### 2. Before Making Changes
```bash
# Profile current state
python3 scripts/profile_embedding_load.py --output reports/before.json
```

### 3. After Making Changes
```bash
# Profile and compare
python3 scripts/profile_embedding_load.py \
  --baseline reports/before.json \
  --output reports/after.json
```

### 4. Verify No Regression
```bash
# Automated check (for CI/CD)
python3 scripts/check_performance_regression.py \
  --current reports/after.json \
  --baseline reports/before.json \
  --threshold 1.2
```

## Integration Points

### Development
- Profile before optimization work
- Profile after to measure impact
- Use recommendations to guide next steps

### CI/CD Pipeline
```yaml
- name: Profile Embedding Load
  run: |
    python3 scripts/profile_embedding_load.py \
      --output reports/current.json

- name: Check Regression
  run: |
    python3 scripts/check_performance_regression.py \
      --current reports/current.json \
      --baseline reports/baseline.json \
      --threshold 1.2
```

### Performance Tracking
- Generate reports periodically
- Store in version control
- Track trends over time
- Identify gradual degradation

## Dependencies

### Required
```bash
pip install sentence-transformers torch
```

### Optional (for memory profiling)
```bash
pip install psutil
```

## Files Created

```
aurora/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ profile_embedding_load.py          # Main profiling tool
â”‚   â”œâ”€â”€ check_performance_regression.py    # Regression checker
â”‚   â”œâ”€â”€ run_embedding_profile.sh           # Shell wrapper
â”‚   â””â”€â”€ README_PROFILING.md                # Quick reference
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ profiling_workflow_example.sh      # Interactive tutorial
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ performance/
â”‚       â””â”€â”€ embedding_load_profiling.md    # Full documentation
â”œâ”€â”€ reports/                                # Generated reports (git-ignored)
â”‚   â”œâ”€â”€ baseline_warm.json
â”‚   â”œâ”€â”€ baseline_cold.json
â”‚   â””â”€â”€ *.json
â”œâ”€â”€ PROFILING_SUMMARY.md                    # Deliverables summary
â””â”€â”€ README_EMBEDDING_PROFILING.md           # This file
```

## Optimization Context

This profiling infrastructure helps measure the impact of:

### Current Optimizations
- **Background loading** (`BackgroundModelLoader` in `model_utils.py`)
- **Lazy initialization** (`EmbeddingProvider` in `embedding_provider.py`)
- **Graceful degradation** (BM25-only fallback in `retrieval.py`)

### Potential Optimizations
- Model quantization (INT8)
- Model pruning
- Persistent model service
- ONNX/TorchScript compilation
- Alternative models (MiniLM-L3 for speed, MPNet for quality)

## Troubleshooting

### "command not found: python"
Use `python3` instead:
```bash
python3 scripts/profile_embedding_load.py
```

### "sentence-transformers not installed"
Install dependencies:
```bash
pip install sentence-transformers torch
```

### High variability in results
- Close other applications
- Increase runs: `--runs 10`
- Check disk I/O (SSD vs HDD)

### Memory shows 0MB
Install psutil:
```bash
pip install psutil
```

## Getting Help

1. **Quick reference**: `cat scripts/README_PROFILING.md`
2. **Full docs**: `cat docs/performance/embedding_load_profiling.md`
3. **Tool help**: `python3 scripts/profile_embedding_load.py --help`
4. **Interactive tutorial**: `./examples/profiling_workflow_example.sh`

## What's Next?

1. âœ… **Run profiling** - Establish your baseline metrics
2. âœ… **Review results** - Understand current performance
3. âœ… **Identify bottlenecks** - Use stage breakdown to find issues
4. âœ… **Implement optimizations** - Try suggestions from recommendations
5. âœ… **Measure impact** - Re-run profiling with baseline comparison
6. âœ… **Track over time** - Add to CI/CD for continuous monitoring

## Success Criteria

Your profiling is successful when:
- âœ… Baseline metrics are established
- âœ… Performance targets are understood
- âœ… Bottlenecks are identified
- âœ… Optimization impact is measured
- âœ… Regressions are prevented via CI/CD

---

**Ready to start?** Run the interactive tutorial:
```bash
./examples/profiling_workflow_example.sh
```

Or dive straight in with:
```bash
python3 scripts/profile_embedding_load.py
```
